"""
–ù–µ–π—Ä–æ—Å–µ—Ç—å –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –Ω–∞–º–µ—Ä–µ–Ω–∏–π –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import json
import os
import re
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
import numpy as np
from collections import defaultdict, Counter
import pickle

@dataclass
class IntentExample:
    """–ü—Ä–∏–º–µ—Ä –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—é –Ω–∞–º–µ—Ä–µ–Ω–∏–π"""
    text: str
    intent: str
    entities: List[Dict[str, str]]
    tokens: List[str] = None
    
    def __post_init__(self):
        if self.tokens is None:
            self.tokens = self.text.lower().split()

@dataclass 
class Entity:
    """–ò–∑–≤–ª–µ—á–µ–Ω–Ω–∞—è —Å—É—â–Ω–æ—Å—Ç—å"""
    text: str
    label: str
    start: int
    end: int
    confidence: float = 1.0

class IntentNERDataset(Dataset):
    """–î–∞—Ç–∞—Å–µ—Ç –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –Ω–∞–º–µ—Ä–µ–Ω–∏–π –∏ NER"""
    
    def __init__(self, examples: List[IntentExample], vocab_size: int = 5000, max_len: int = 50):
        self.examples = examples
        self.max_len = max_len
        
        # –°—Ç—Ä–æ–∏–º —Å–ª–æ–≤–∞—Ä—å
        self.word2idx, self.idx2word = self._build_vocab(vocab_size)
        
        # –°–ª–æ–≤–∞—Ä—å –∏–Ω—Ç–µ–Ω—Ç–æ–≤
        self.intent2idx = {}
        self.idx2intent = {}
        self._build_intent_vocab()
        
        # –°–ª–æ–≤–∞—Ä—å —Å—É—â–Ω–æ—Å—Ç–µ–π
        self.entity_labels = ['O', 'B-PROGRAM', 'I-PROGRAM', 'B-FILE', 'I-FILE',
                            'B-DIRECTORY', 'I-DIRECTORY', 'B-TEXT', 'I-TEXT',
                            'B-QUERY', 'I-QUERY', 'B-URL', 'I-URL', 'B-NUMBER',
                            'I-NUMBER', 'B-DATETIME', 'I-DATETIME']
        self.entity2idx = {label: idx for idx, label in enumerate(self.entity_labels)}
        self.idx2entity = {idx: label for label, idx in self.entity2idx.items()}
    
    def _build_vocab(self, vocab_size: int) -> Tuple[Dict[str, int], Dict[int, str]]:
        """–°—Ç—Ä–æ–∏—Ç —Å–ª–æ–≤–∞—Ä—å —Å–ª–æ–≤"""
        word_counts = Counter()
        
        for example in self.examples:
            word_counts.update(example.tokens)
        
        # –ë–µ—Ä–µ–º —Å–∞–º—ã–µ —á–∞—Å—Ç—ã–µ —Å–ª–æ–≤–∞
        most_common = word_counts.most_common(vocab_size - 2)  # -2 –¥–ª—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
        
        word2idx = {'<PAD>': 0, '<UNK>': 1}
        idx2word = {0: '<PAD>', 1: '<UNK>'}
        
        for idx, (word, _) in enumerate(most_common, start=2):
            word2idx[word] = idx
            idx2word[idx] = word
        
        return word2idx, idx2word
    
    def _build_intent_vocab(self):
        """–°—Ç—Ä–æ–∏—Ç —Å–ª–æ–≤–∞—Ä—å –∏–Ω—Ç–µ–Ω—Ç–æ–≤"""
        intents = set(example.intent for example in self.examples)
        self.intent2idx = {intent: idx for idx, intent in enumerate(intents)}
        self.idx2intent = {idx: intent for intent, idx in self.intent2idx.items()}
    
    def text_to_indices(self, text: str) -> List[int]:
        """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Ç–µ–∫—Å—Ç –≤ –∏–Ω–¥–µ–∫—Å—ã"""
        tokens = text.lower().split()[:self.max_len]
        indices = [self.word2idx.get(token, self.word2idx['<UNK>']) for token in tokens]
        
        # –ü–∞–¥–¥–∏–Ω–≥
        if len(indices) < self.max_len:
            indices += [self.word2idx['<PAD>']] * (self.max_len - len(indices))
        
        return indices[:self.max_len]
    
    def create_entity_labels(self, text: str, entities: List[Dict]) -> List[int]:
        """–°–æ–∑–¥–∞–µ—Ç –º–µ—Ç–∫–∏ —Å—É—â–Ω–æ—Å—Ç–µ–π –¥–ª—è —Ç–µ–∫—Å—Ç–∞"""
        tokens = text.lower().split()[:self.max_len]
        labels = [0] * len(tokens)  # 0 = 'O' (–Ω–µ —Å—É—â–Ω–æ—Å—Ç—å)
        
        # –°–æ–ø–æ—Å—Ç–∞–≤–ª—è–µ–º —Å—É—â–Ω–æ—Å—Ç–∏ —Å —Ç–æ–∫–µ–Ω–∞–º–∏
        for entity in entities:
            entity_text = entity.get('text', '').lower()
            entity_label = entity.get('label', 'O')
            
            # –ò—â–µ–º —Å—É—â–Ω–æ—Å—Ç—å –≤ —Ç–µ–∫—Å—Ç–µ
            if entity_text in text.lower():
                # –ù–∞—Ö–æ–¥–∏–º –ø–æ–∑–∏—Ü–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤ —Å—É—â–Ω–æ—Å—Ç–∏
                entity_tokens = entity_text.split()
                text_tokens = text.lower().split()
                
                for i in range(len(text_tokens) - len(entity_tokens) + 1):
                    if text_tokens[i:i+len(entity_tokens)] == entity_tokens:
                        # –ü–æ–º–µ—á–∞–µ–º –Ω–∞—á–∞–ª–æ —Å—É—â–Ω–æ—Å—Ç–∏
                        if entity_label != 'O':
                            labels[i] = self.entity2idx.get(f'B-{entity_label}', 0)
                            # –ü–æ–º–µ—á–∞–µ–º –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–∏
                            for j in range(1, len(entity_tokens)):
                                if i + j < len(labels):
                                    labels[i+j] = self.entity2idx.get(f'I-{entity_label}', 0)
                        break
        
        # –ü–∞–¥–¥–∏–Ω–≥
        if len(labels) < self.max_len:
            labels += [0] * (self.max_len - len(labels))
        
        return labels[:self.max_len]
    
    def __len__(self):
        return len(self.examples)
    
    def __getitem__(self, idx):
        example = self.examples[idx]
        
        # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç
        text_indices = self.text_to_indices(example.text)
        
        # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç–∫—É –∏–Ω—Ç–µ–Ω—Ç–∞
        intent_idx = self.intent2idx.get(example.intent, 0)
        
        # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç–∫–∏ —Å—É—â–Ω–æ—Å—Ç–µ–π
        entity_labels = self.create_entity_labels(example.text, example.entities)
        
        return {
            'text': torch.tensor(text_indices, dtype=torch.long),
            'intent': torch.tensor(intent_idx, dtype=torch.long),
            'entities': torch.tensor(entity_labels, dtype=torch.long),
            'text_str': example.text,
            'intent_str': example.intent
        }

class JointIntentNERModel(nn.Module):
    """–°–æ–≤–º–µ—Å—Ç–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –Ω–∞–º–µ—Ä–µ–Ω–∏–π –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π"""
    
    def __init__(self, vocab_size: int, num_intents: int, num_entities: int,
                 embedding_dim: int = 128, hidden_dim: int = 256):
        super().__init__()
        
        # –°–ª–æ–π —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)
        
        # BiLSTM –¥–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
        self.bilstm = nn.LSTM(
            input_size=embedding_dim,
            hidden_size=hidden_dim // 2,
            num_layers=2,
            batch_first=True,
            bidirectional=True,
            dropout=0.3
        )
        
        # –í–Ω–∏–º–∞–Ω–∏–µ –¥–ª—è –∏–Ω—Ç–µ–Ω—Ç–æ–≤
        self.intent_attention = nn.MultiheadAttention(
            embed_dim=hidden_dim,
            num_heads=4,
            dropout=0.3,
            batch_first=True
        )
        
        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∏–Ω—Ç–µ–Ω—Ç–æ–≤
        self.intent_classifier = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim // 2, num_intents)
        )
        
        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å—É—â–Ω–æ—Å—Ç–µ–π (CRF –∏–ª–∏ –ª–∏–Ω–µ–π–Ω—ã–π)
        self.entity_classifier = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim // 2, num_entities)
        )
        
        # –°–ª–æ–π –¥–ª—è –Ω–æ–≤—ã—Ö –∏–Ω—Ç–µ–Ω—Ç–æ–≤
        self.new_intent_projection = nn.Linear(hidden_dim, 64)
        
    def forward(self, text_ids, return_attentions=False):
        # –≠–º–±–µ–¥–¥–∏–Ω–≥–∏
        embedded = self.embedding(text_ids)
        
        # BiLSTM
        lstm_out, _ = self.bilstm(embedded)
        
        # –í–Ω–∏–º–∞–Ω–∏–µ –¥–ª—è –∏–Ω—Ç–µ–Ω—Ç–æ–≤
        intent_attn_out, intent_attn_weights = self.intent_attention(
            lstm_out, lstm_out, lstm_out
        )
        
        # –ü—É–ª–∏–Ω–≥ –¥–ª—è –∏–Ω—Ç–µ–Ω—Ç–∞ (–≤–∑–≤–µ—à–µ–Ω–Ω–æ–µ —Å—Ä–µ–¥–Ω–µ–µ)
        attention_weights = F.softmax(intent_attn_weights.mean(dim=1), dim=-1)
        intent_context = torch.bmm(attention_weights.unsqueeze(1), intent_attn_out).squeeze(1)
        
        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏–Ω—Ç–µ–Ω—Ç–∞
        intent_logits = self.intent_classifier(intent_context)
        
        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π
        entity_logits = self.entity_classifier(lstm_out)
        
        # –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –Ω–æ–≤—ã—Ö –∏–Ω—Ç–µ–Ω—Ç–æ–≤
        new_intent_embedding = self.new_intent_projection(intent_context)
        
        outputs = {
            'intent_logits': intent_logits,
            'entity_logits': entity_logits,
            'intent_embedding': new_intent_embedding
        }
        
        if return_attentions:
            outputs['attention_weights'] = intent_attn_weights
        
        return outputs

class NeuralIntentRecognizer:
    """
    –ù–µ–π—Ä–æ—Å–µ—Ç–µ–≤–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –Ω–∞–º–µ—Ä–µ–Ω–∏–π –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π
    """
    
    def __init__(self, model_path: str, data_path: Optional[str] = None):
        self.model_path = Path(model_path)
        self.data_path = Path(data_path) if data_path else self.model_path.parent / "intent_data.json"
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
        self.vocab_size = 5000
        self.embedding_dim = 128
        self.hidden_dim = 256
        self.max_len = 50
        
        # –ú–æ–¥–µ–ª—å –∏ –¥–∞–Ω–Ω—ã–µ
        self.model = None
        self.dataset = None
        self.intent_labels = []
        self.entity_labels = []
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            'total_predictions': 0,
            'high_confidence': 0,
            'low_confidence': 0,
            'unknown_intents': 0
        }
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–ª–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º
        self._load_or_initialize()
        
        # –ü—Ä–∞–≤–∏–ª–∞ –¥–ª—è –±–∞–∑–æ–≤—ã—Ö –∏–Ω—Ç–µ–Ω—Ç–æ–≤ (–µ—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞)
        self.basic_intent_rules = self._load_basic_rules()
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —Å—É—â–Ω–æ—Å—Ç–µ–π
        self.entity_patterns = self._load_entity_patterns()
    
    def _load_or_initialize(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏–ª–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª—å"""
        if self.model_path.exists():
            print(f"üìÇ –ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –Ω–∞–º–µ—Ä–µ–Ω–∏–π –∏–∑ {self.model_path}")
            self._load_model()
        else:
            print("üÜï –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –Ω–∞–º–µ—Ä–µ–Ω–∏–π")
            self._initialize_model()
        
        if self.data_path and self.data_path.exists():
            self._load_training_data()
    
    def _initialize_model(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å"""
        # –ë–∞–∑–æ–≤—ã–µ –∏–Ω—Ç–µ–Ω—Ç—ã
        base_intents = [
            'open_program', 'type_text', 'search_web', 'create_file',
            'delete_file', 'copy_text', 'paste_text', 'save_file',
            'get_time', 'list_files', 'create_folder', 'take_screenshot',
            'system_info', 'greeting', 'goodbye', 'help',
            'unknown', 'learn_command'
        ]
        
        # –ë–∞–∑–æ–≤—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏
        entity_labels = ['O', 'B-PROGRAM', 'I-PROGRAM', 'B-FILE', 'I-FILE',
                        'B-DIRECTORY', 'I-DIRECTORY', 'B-TEXT', 'I-TEXT',
                        'B-QUERY', 'I-QUERY', 'B-URL', 'I-URL']
        
        self.intent_labels = base_intents
        self.entity_labels = entity_labels
        
        # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
        self.model = JointIntentNERModel(
            vocab_size=self.vocab_size,
            num_intents=len(self.intent_labels),
            num_entities=len(self.entity_labels),
            embedding_dim=self.embedding_dim,
            hidden_dim=self.hidden_dim
        )
        
        print(f"‚úÖ –ú–æ–¥–µ–ª—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞: {len(self.intent_labels)} –∏–Ω—Ç–µ–Ω—Ç–æ–≤, {len(self.entity_labels)} —Å—É—â–Ω–æ—Å—Ç–µ–π")
    
    def _load_model(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å"""
        try:
            checkpoint = torch.load(self.model_path, map_location='cpu')
            
            self.intent_labels = checkpoint['intent_labels']
            self.entity_labels = checkpoint['entity_labels']
            
            self.model = JointIntentNERModel(
                vocab_size=checkpoint['vocab_size'],
                num_intents=len(self.intent_labels),
                num_entities=len(self.entity_labels),
                embedding_dim=checkpoint['embedding_dim'],
                hidden_dim=checkpoint['hidden_dim']
            )
            
            self.model.load_state_dict(checkpoint['model_state'])
            self.model.eval()
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –µ—Å–ª–∏ –µ—Å—Ç—å
            if 'word2idx' in checkpoint:
                self.word2idx = checkpoint['word2idx']
                self.idx2word = checkpoint['idx2word']
            
            print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {len(self.intent_labels)} –∏–Ω—Ç–µ–Ω—Ç–æ–≤")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            self._initialize_model()
    
    def _load_training_data(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        if not self.data_path.exists():
            self.dataset = None
            return
        
        try:
            with open(self.data_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            examples = []
            for item in data:
                example = IntentExample(
                    text=item['text'],
                    intent=item['intent'],
                    entities=item.get('entities', [])
                )
                examples.append(example)
            
            if examples:
                self.dataset = IntentNERDataset(examples, self.vocab_size, self.max_len)
                print(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(examples)} –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
                
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
            self.dataset = None
    
    def _load_basic_rules(self) -> Dict[str, List[str]]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø—Ä–∞–≤–∏–ª–∞ –¥–ª—è –±–∞–∑–æ–≤—ã—Ö –∏–Ω—Ç–µ–Ω—Ç–æ–≤"""
        return {
            'open_program': ['–æ—Ç–∫—Ä–æ–π', '–∑–∞–ø—É—Å—Ç–∏', '–æ—Ç–∫—Ä—ã—Ç—å', '–∑–∞–ø—É—Å—Ç–∏—Ç—å', '–≤–∫–ª—é—á–∏'],
            'type_text': ['–Ω–∞–ø–µ—á–∞—Ç–∞–π', '–Ω–∞–ø–∏—à–∏', '–≤–≤–µ–¥–∏', '–ø–µ—á–∞—Ç–∞–π', '–≤–≤–æ–¥'],
            'search_web': ['–Ω–∞–π–¥–∏', '–ø–æ–∏—â–∏', '–∏—â–∏', '–ø–æ–∏—Å–∫', '–Ω–∞–π—Ç–∏'],
            'create_file': ['—Å–æ–∑–¥–∞–π', '—Å–¥–µ–ª–∞–π', '—Å–æ–∑–¥–∞—Ç—å', '–Ω–æ–≤—ã–π —Ñ–∞–π–ª'],
            'delete_file': ['—É–¥–∞–ª–∏', '—Å—Ç–µ—Ä–µ—Ç—å', '—É–¥–∞–ª–∏—Ç—å', '—É–Ω–∏—á—Ç–æ–∂—å'],
            'get_time': ['–≤—Ä–µ–º—è', '–∫–æ—Ç–æ—Ä—ã–π —á–∞—Å', '—Å–∫–æ–ª—å–∫–æ –≤—Ä–µ–º–µ–Ω–∏', '–≤—Ä–µ–º–µ–Ω–∏'],
            'greeting': ['–ø—Ä–∏–≤–µ—Ç', '–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π', '–¥–æ–±—Ä—ã–π', 'hello', '—Ö–∞–π'],
            'goodbye': ['–ø–æ–∫–∞', '–¥–æ —Å–≤–∏–¥–∞–Ω–∏—è', '–ø—Ä–æ—â–∞–π', '–≤—ã—Ö–æ–¥', '—Å—Ç–æ–ø'],
            'help': ['–ø–æ–º–æ—â—å', '–ø–æ–º–æ–≥–∏', '—á—Ç–æ —Ç—ã —É–º–µ–µ—à—å', '–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏'],
            'learn_command': ['–Ω–∞—É—á–∏', '–∑–∞–ø–æ–º–Ω–∏', '–≤—ã—É—á–∏', '–æ–±—É—á–∏']
        }
    
    def _load_entity_patterns(self) -> Dict[str, List[str]]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π"""
        return {
            'PROGRAM': ['–±—Ä–∞—É–∑–µ—Ä', '—Ö—Ä–æ–º', '–±–ª–æ–∫–Ω–æ—Ç', '–∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä', '–ø—Ä–æ–≤–æ–¥–Ω–∏–∫',
                       '—Ç–µ—Ä–º–∏–Ω–∞–ª', 'word', 'excel', '–ø–∞–Ω–µ–ª—å', 'notepad'],
            'FILE': ['.txt', '.doc', '.docx', '.pdf', '.jpg', '.png', '—Ñ–∞–π–ª'],
            'DIRECTORY': ['–ø–∞–ø–∫–∞', '–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è', '–∫–∞—Ç–∞–ª–æ–≥', 'folder'],
            'TEXT': ['—Ç–µ–∫—Å—Ç', '—Å–æ–æ–±—â–µ–Ω–∏–µ', '–∑–∞–ø–∏—Å—å', 'note'],
            'QUERY': ['–∑–∞–ø—Ä–æ—Å', '–≤–æ–ø—Ä–æ—Å', '–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è', '—á—Ç–æ —Ç–∞–∫–æ–µ'],
            'URL': ['http://', 'https://', 'www.', '.ru', '.com'],
            'NUMBER': ['1', '2', '3', '4', '5', '6', '7', '8', '9', '0'],
            'DATETIME': ['—Å–µ–≥–æ–¥–Ω—è', '–∑–∞–≤—Ç—Ä–∞', '–≤—á–µ—Ä–∞', '—á–∞—Å', '–º–∏–Ω—É—Ç–∞', '—Å–µ–∫—É–Ω–¥–∞']
        }
    
    def predict(self, text: str) -> Dict[str, Any]:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞–º–µ—Ä–µ–Ω–∏–µ –∏ —Å—É—â–Ω–æ—Å—Ç–∏ –¥–ª—è —Ç–µ–∫—Å—Ç–∞
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è
        """
        self.stats['total_predictions'] += 1
        
        # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª–∞
        if self.model is None or self.dataset is None:
            return self._predict_with_rules(text)
        
        try:
            # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç
            text_indices = self._text_to_indices(text)
            text_tensor = torch.tensor([text_indices], dtype=torch.long)
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            with torch.no_grad():
                self.model.eval()
                outputs = self.model(text_tensor)
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏–Ω—Ç–µ–Ω—Ç
                intent_logits = outputs['intent_logits']
                intent_probs = F.softmax(intent_logits, dim=1)
                confidence, intent_idx = torch.max(intent_probs, dim=1)
                
                confidence = confidence.item()
                intent_idx = intent_idx.item()
                
                # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç–∫—É –∏–Ω—Ç–µ–Ω—Ç–∞
                if intent_idx < len(self.intent_labels):
                    intent = self.intent_labels[intent_idx]
                else:
                    intent = 'unknown'
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—É—â–Ω–æ—Å—Ç–∏
                entity_logits = outputs['entity_logits']
                entity_probs = F.softmax(entity_logits, dim=2)
                _, entity_idxs = torch.max(entity_probs, dim=2)
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—É—â–Ω–æ—Å—Ç–∏
                entities = self._extract_entities(text, entity_idxs[0].tolist())
                
                # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
                if confidence > 0.7:
                    self.stats['high_confidence'] += 1
                else:
                    self.stats['low_confidence'] += 1
                
                if intent == 'unknown':
                    self.stats['unknown_intents'] += 1
                
                return {
                    'intent': intent,
                    'confidence': confidence,
                    'entities': entities,
                    'intent_probs': intent_probs.tolist()[0],
                    'method': 'neural'
                }
                
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {e}")
            return self._predict_with_rules(text)
    
    def _predict_with_rules(self, text: str) -> Dict[str, Any]:
        """–ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —Å –ø–æ–º–æ—â—å—é –ø—Ä–∞–≤–∏–ª (fallback)"""
        text_lower = text.lower()
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏–Ω—Ç–µ–Ω—Ç –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        intent = 'unknown'
        confidence = 0.5
        max_matches = 0
        
        for intent_name, keywords in self.basic_intent_rules.items():
            matches = sum(1 for keyword in keywords if keyword in text_lower)
            
            if matches > max_matches:
                max_matches = matches
                intent = intent_name
                confidence = min(0.3 + matches * 0.2, 0.9)  # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—É—â–Ω–æ—Å—Ç–∏ –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º
        entities = self._extract_entities_with_patterns(text)
        
        return {
            'intent': intent,
            'confidence': confidence,
            'entities': entities,
            'method': 'rules'
        }
    
    def _text_to_indices(self, text: str) -> List[int]:
        """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Ç–µ–∫—Å—Ç –≤ –∏–Ω–¥–µ–∫—Å—ã"""
        if not hasattr(self, 'dataset') or self.dataset is None:
            # –ü—Ä–æ—Å—Ç–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –µ—Å–ª–∏ –Ω–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç–∞
            tokens = text.lower().split()[:self.max_len]
            
            if hasattr(self, 'word2idx'):
                indices = [self.word2idx.get(token, 1) for token in tokens]
            else:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç–æ–π —Ö—ç—à
                indices = []
                for token in tokens:
                    token_hash = hash(token) % (self.vocab_size - 2) + 2
                    indices.append(token_hash)
            
            # –ü–∞–¥–¥–∏–Ω–≥
            if len(indices) < self.max_len:
                indices += [0] * (self.max_len - len(indices))
            
            return indices[:self.max_len]
        else:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–∞—Ç–∞—Å–µ—Ç
            return self.dataset.text_to_indices(text)
    
    def _extract_entities(self, text: str, entity_labels: List[int]) -> List[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—É—â–Ω–æ—Å—Ç–∏ –∏–∑ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫"""
        if not hasattr(self, 'dataset') or self.dataset is None:
            return self._extract_entities_with_patterns(text)
        
        tokens = text.lower().split()
        entities = []
        current_entity = None
        
        for i, label_idx in enumerate(entity_labels[:len(tokens)]):
            if label_idx >= len(self.entity_labels):
                continue
            
            label = self.entity_labels[label_idx]
            
            if label == 'O':
                if current_entity:
                    entities.append(current_entity)
                    current_entity = None
            elif label.startswith('B-'):
                if current_entity:
                    entities.append(current_entity)
                
                entity_type = label[2:]  # –£–±–∏—Ä–∞–µ–º B-
                current_entity = {
                    'text': tokens[i],
                    'label': entity_type,
                    'start': i,
                    'end': i + 1,
                    'confidence': 0.8
                }
            elif label.startswith('I-') and current_entity:
                # –ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–∏
                current_entity['text'] += ' ' + tokens[i]
                current_entity['end'] = i + 1
        
        if current_entity:
            entities.append(current_entity)
        
        # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç —Å—É—â–Ω–æ—Å—Ç–∏
        original_tokens = text.split()
        for entity in entities:
            entity_text = ' '.join(original_tokens[entity['start']:entity['end']])
            entity['text'] = entity_text
        
        return entities
    
    def _extract_entities_with_patterns(self, text: str) -> List[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—É—â–Ω–æ—Å—Ç–∏ —Å –ø–æ–º–æ—â—å—é –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
        text_lower = text.lower()
        entities = []
        
        for entity_type, patterns in self.entity_patterns.items():
            for pattern in patterns:
                if pattern in text_lower:
                    # –ù–∞—Ö–æ–¥–∏–º –ø–æ–∑–∏—Ü–∏—é
                    idx = text_lower.find(pattern)
                    
                    entities.append({
                        'text': text[idx:idx+len(pattern)],
                        'label': entity_type,
                        'start': idx,
                        'end': idx + len(pattern),
                        'confidence': 0.7,
                        'method': 'pattern'
                    })
        
        # –£–±–∏—Ä–∞–µ–º –ø–µ—Ä–µ—Å–µ–∫–∞—é—â–∏–µ—Å—è —Å—É—â–Ω–æ—Å—Ç–∏
        if entities:
            entities.sort(key=lambda x: x['start'])
            filtered = [entities[0]]
            
            for entity in entities[1:]:
                if entity['start'] >= filtered[-1]['end']:
                    filtered.append(entity)
            
            entities = filtered
        
        return entities
    
    def train_on_example(self, text: str, intent: str, entities: List[Dict[str, str]]):
        """
        –û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å –Ω–∞ –Ω–æ–≤–æ–º –ø—Ä–∏–º–µ—Ä–µ
        
        Args:
            text: –¢–µ–∫—Å—Ç –∫–æ–º–∞–Ω–¥—ã
            intent: –ù–∞–º–µ—Ä–µ–Ω–∏–µ
            entities: –°–ø–∏—Å–æ–∫ —Å—É—â–Ω–æ—Å—Ç–µ–π
        """
        print(f"üéì –û–±—É—á–∞—é –º–æ–¥–µ–ª—å –Ω–∞ –ø—Ä–∏–º–µ—Ä–µ: '{text}' -> {intent}")
        
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–º–µ—Ä
        example = IntentExample(
            text=text,
            intent=intent,
            entities=entities
        )
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –¥–∞—Ç–∞—Å–µ—Ç
        if self.dataset is None:
            examples = [example]
            self.dataset = IntentNERDataset(examples, self.vocab_size, self.max_len)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –∏–Ω—Ç–µ–Ω—Ç—ã
            if intent not in self.intent_labels:
                self.intent_labels.append(intent)
        else:
            # –î–æ–±–∞–≤–ª—è–µ–º –∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º –ø—Ä–∏–º–µ—Ä–∞–º
            examples = self.dataset.examples.copy()
            examples.append(example)
            self.dataset = IntentNERDataset(examples, self.vocab_size, self.max_len)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –∏–Ω—Ç–µ–Ω—Ç—ã
            if intent not in self.intent_labels:
                self.intent_labels.append(intent)
        
        # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ —Å–æ–∑–¥–∞–Ω–∞, —Å–æ–∑–¥–∞–µ–º
        if self.model is None:
            self._initialize_model()
        
        # –ü–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å —Å –Ω–æ–≤—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –∏–Ω—Ç–µ–Ω—Ç–æ–≤
        self.model = JointIntentNERModel(
            vocab_size=self.vocab_size,
            num_intents=len(self.intent_labels),
            num_entities=len(self.dataset.entity_labels),
            embedding_dim=self.embedding_dim,
            hidden_dim=self.hidden_dim
        )
        
        # –û–±—É—á–∞–µ–º –Ω–∞ –æ–¥–Ω–æ–º –ø—Ä–∏–º–µ—Ä–µ
        self._train_one_epoch()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
        self.save_model()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ
        self._save_training_example(example)
        
        print(f"‚úÖ –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –Ω–∞ –ø—Ä–∏–º–µ—Ä–µ")
    
    def _train_one_epoch(self):
        """–û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å –Ω–∞ –æ–¥–Ω–æ–π —ç–ø–æ—Ö–µ"""
        if self.dataset is None or len(self.dataset) == 0:
            return
        
        dataloader = DataLoader(
            self.dataset,
            batch_size=2,
            shuffle=True
        )
        
        criterion_intent = nn.CrossEntropyLoss()
        criterion_entity = nn.CrossEntropyLoss()
        optimizer = optim.AdamW(self.model.parameters(), lr=0.001)
        
        self.model.train()
        total_loss = 0
        
        for batch in dataloader:
            optimizer.zero_grad()
            
            text_ids = batch['text']
            intent_labels = batch['intent']
            entity_labels = batch['entities']
            
            # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥
            outputs = self.model(text_ids)
            
            # –í—ã—á–∏—Å–ª—è–µ–º –ø–æ—Ç–µ—Ä–∏
            loss_intent = criterion_intent(outputs['intent_logits'], intent_labels)
            loss_entity = criterion_entity(
                outputs['entity_logits'].view(-1, outputs['entity_logits'].size(-1)),
                entity_labels.view(-1)
            )
            
            loss = loss_intent + loss_entity
            
            # –û–±—Ä–∞—Ç–Ω—ã–π –ø—Ä–æ—Ö–æ–¥
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
        
        avg_loss = total_loss / len(dataloader) if len(dataloader) > 0 else 0
        print(f"  –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏: Loss = {avg_loss:.4f}")
    
    def save_model(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–æ–¥–µ–ª—å"""
        checkpoint = {
            'model_state': self.model.state_dict() if self.model else None,
            'intent_labels': self.intent_labels,
            'entity_labels': self.entity_labels,
            'vocab_size': self.vocab_size,
            'embedding_dim': self.embedding_dim,
            'hidden_dim': self.hidden_dim
        }
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ª–æ–≤–∞—Ä—å –µ—Å–ª–∏ –µ—Å—Ç—å
        if hasattr(self, 'dataset') and self.dataset:
            checkpoint['word2idx'] = self.dataset.word2idx
            checkpoint['idx2word'] = self.dataset.idx2word
        
        torch.save(checkpoint, self.model_path)
        print(f"üíæ –ú–æ–¥–µ–ª—å —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")
    
    def _save_training_example(self, example: IntentExample):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø—Ä–∏–º–µ—Ä –æ–±—É—á–µ–Ω–∏—è"""
        if not self.data_path:
            return
        
        data = []
        if self.data_path.exists():
            try:
                with open(self.data_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
            except:
                pass
        
        data.append({
            'text': example.text,
            'intent': example.intent,
            'entities': example.entities,
            'timestamp': time.time() if 'time' in globals() else 0
        })
        
        with open(self.data_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    
    def get_statistics(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
        return {
            **self.stats,
            'total_intents': len(self.intent_labels),
            'total_entities': len(self.entity_labels),
            'has_model': self.model is not None,
            'has_dataset': self.dataset is not None and len(self.dataset) > 0
        }

# –£—Ç–∏–ª–∏—Ç–∞ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
def test_intent_recognizer():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç–µ–ª—å –Ω–∞–º–µ—Ä–µ–Ω–∏–π"""
    print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç–µ–ª—è –Ω–∞–º–µ—Ä–µ–Ω–∏–π...")
    
    import tempfile
    import shutil
    
    # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
    temp_dir = tempfile.mkdtemp()
    model_path = Path(temp_dir) / "intent_model.pt"
    data_path = Path(temp_dir) / "intent_data.json"
    
    try:
        # –°–æ–∑–¥–∞–µ–º —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç–µ–ª—å
        recognizer = NeuralIntentRecognizer(str(model_path), str(data_path))
        
        # –¢–µ—Å—Ç 1: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –ø—Ä–∞–≤–∏–ª–∞–º–∏ (–º–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞)
        print("\n1. –¢–µ—Å—Ç–∏—Ä—É—é –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –ø—Ä–∞–≤–∏–ª–∞–º–∏...")
        
        test_texts = [
            "–æ—Ç–∫—Ä–æ–π –±—Ä–∞—É–∑–µ—Ä –ø–æ–∂–∞–ª—É–π—Å—Ç–∞",
            "–Ω–∞–ø–µ—á–∞—Ç–∞–π —Ç–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞",
            "—Å–∫–æ–ª—å–∫–æ —Å–µ–π—á–∞—Å –≤—Ä–µ–º–µ–Ω–∏",
            "–ø—Ä–∏–≤–µ—Ç –∫–∞–∫ –¥–µ–ª–∞",
            "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –∫–æ–º–∞–Ω–¥–∞ –¥–ª—è —Ç–µ—Å—Ç–∞"
        ]
        
        for text in test_texts:
            result = recognizer.predict(text)
            print(f"  üìù '{text[:30]}...' ‚Üí {result['intent']} ({result['confidence']:.2%})")
            if result['entities']:
                print(f"    –°—É—â–Ω–æ—Å—Ç–∏: {result['entities']}")
        
        # –¢–µ—Å—Ç 2: –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        print("\n2. –¢–µ—Å—Ç–∏—Ä—É—é –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
        
        training_examples = [
            ("–æ—Ç–∫—Ä–æ–π –∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä", "open_program", [{"text": "–∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä", "label": "PROGRAM"}]),
            ("—Å–æ–∑–¥–∞–π —Ñ–∞–π–ª –æ—Ç—á–µ—Ç.txt", "create_file", [{"text": "–æ—Ç—á–µ—Ç.txt", "label": "FILE"}]),
            ("–Ω–∞–π–¥–∏ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ python", "search_web", [{"text": "python", "label": "QUERY"}])
        ]
        
        for text, intent, entities in training_examples:
            recognizer.train_on_example(text, intent, entities)
        
        # –¢–µ—Å—Ç 3: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é
        print("\n3. –¢–µ—Å—Ç–∏—Ä—É—é –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é...")
        
        for text in test_texts:
            result = recognizer.predict(text)
            print(f"  üß† '{text[:30]}...' ‚Üí {result['intent']} ({result['confidence']:.2%}, –º–µ—Ç–æ–¥: {result['method']})")
        
        # –¢–µ—Å—Ç 4: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        print("\n4. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞...")
        
        stats = recognizer.get_statistics()
        for key, value in stats.items():
            if isinstance(value, (int, float)):
                print(f"  üìä {key}: {value}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
        recognizer.save_model()
        
    finally:
        # –û—á–∏—Å—Ç–∫–∞
        shutil.rmtree(temp_dir)
        print("\nüßπ –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã —É–¥–∞–ª–µ–Ω—ã")

if __name__ == "__main__":
    import time
    test_intent_recognizer()