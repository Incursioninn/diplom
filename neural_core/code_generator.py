"""
–ù–µ–π—Ä–æ—Å–µ—Ç—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ Python-–∫–æ–¥–∞ –ø–æ –æ–ø–∏—Å–∞–Ω–∏—é –∫–æ–º–∞–Ω–¥—ã
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import json
import os
import re
import ast
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
import numpy as np
from dataclasses import dataclass
from collections import OrderedDict

@dataclass
class CodeExample:
    """–ü—Ä–∏–º–µ—Ä –∫–æ–¥–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
    description: str
    code: str
    intent_type: str
    complexity: int  # 1-–ø—Ä–æ—Å—Ç–æ–π, 2-—Å—Ä–µ–¥–Ω–∏–π, 3-—Å–ª–æ–∂–Ω—ã–π

class CodeDataset(Dataset):
    """–î–∞—Ç–∞—Å–µ—Ç –ø–∞—Ä (–æ–ø–∏—Å–∞–Ω–∏–µ ‚Üí –∫–æ–¥) –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
    
    def __init__(self, examples: List[CodeExample], vocab_size: int = 5000, max_len: int = 200):
        self.examples = examples
        self.vocab_size = vocab_size
        self.max_len = max_len
        
        # –°–ª–æ–≤–∞—Ä–∏ –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏
        self.char_to_idx = {}
        self.idx_to_char = {}
        self._build_char_vocab()
        
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–ª—è Python –∫–æ–¥–∞
        self.code_tokens = set()
        self._build_code_vocab()
    
    def _build_char_vocab(self):
        """–°—Ç—Ä–æ–∏—Ç —Å–ª–æ–≤–∞—Ä—å —Å–∏–º–≤–æ–ª–æ–≤"""
        chars = set()
        
        for example in self.examples:
            chars.update(example.description)
            chars.update(example.code)
        
        # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
        special_tokens = ['<PAD>', '<SOS>', '<EOS>', '<UNK>']
        for token in special_tokens:
            chars.add(token)
        
        # –°–æ–∑–¥–∞–µ–º –º–∞–ø–ø–∏–Ω–≥
        self.char_to_idx = {char: idx for idx, char in enumerate(sorted(chars))}
        self.idx_to_char = {idx: char for char, idx in self.char_to_idx.items()}
        
        # –ï—Å–ª–∏ —Å–∏–º–≤–æ–ª–æ–≤ –±–æ–ª—å—à–µ vocab_size, –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º
        if len(self.char_to_idx) > self.vocab_size:
            # –û—Å—Ç–∞–≤–ª—è–µ–º —Å–∞–º—ã–µ —á–∞—Å—Ç—ã–µ —Å–∏–º–≤–æ–ª—ã
            char_counts = {}
            for example in self.examples:
                for char in example.description + example.code:
                    char_counts[char] = char_counts.get(char, 0) + 1
            
            sorted_chars = sorted(char_counts.items(), key=lambda x: x[1], reverse=True)
            top_chars = [char for char, _ in sorted_chars[:self.vocab_size - len(special_tokens)]]
            
            self.char_to_idx = {char: idx for idx, char in enumerate(special_tokens + top_chars)}
            self.idx_to_char = {idx: char for char, idx in self.char_to_idx.items()}
    
    def _build_code_vocab(self):
        """–°—Ç—Ä–æ–∏—Ç —Å–ª–æ–≤–∞—Ä—å –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ Python"""
        python_keywords = [
            'def', 'return', 'import', 'from', 'as', 'if', 'else', 'elif',
            'for', 'while', 'try', 'except', 'with', 'class', 'self'
        ]
        
        python_stdlib = [
            'os', 'sys', 'subprocess', 'time', 'datetime', 'json',
            'shutil', 'pathlib', 're', 'typing', 'webbrowser'
        ]
        
        self.code_tokens = set(python_keywords + python_stdlib)
    
    def encode_text(self, text: str, add_special: bool = True) -> List[int]:
        """–ö–æ–¥–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç –≤ –∏–Ω–¥–µ–∫—Å—ã"""
        indices = []
        
        if add_special:
            indices.append(self.char_to_idx.get('<SOS>', 1))
        
        for char in text[:self.max_len - 2]:
            indices.append(self.char_to_idx.get(char, self.char_to_idx.get('<UNK>', 3)))
        
        if add_special:
            indices.append(self.char_to_idx.get('<EOS>', 2))
        
        # –ü–∞–¥–¥–∏–Ω–≥
        if len(indices) < self.max_len:
            indices += [self.char_to_idx['<PAD>']] * (self.max_len - len(indices))
        
        return indices[:self.max_len]
    
    def __len__(self):
        return len(self.examples)
    
    def __getitem__(self, idx):
        example = self.examples[idx]
        
        # –ö–æ–¥–∏—Ä—É–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –∏ –∫–æ–¥
        desc_encoded = self.encode_text(example.description, add_special=True)
        code_encoded = self.encode_text(example.code, add_special=True)
        
        return {
            'description': torch.tensor(desc_encoded, dtype=torch.long),
            'code': torch.tensor(code_encoded, dtype=torch.long),
            'intent_type': example.intent_type,
            'complexity': example.complexity
        }

class CodeGeneratorModel(nn.Module):
    """–ú–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞ (seq2seq —Å –≤–Ω–∏–º–∞–Ω–∏–µ–º)"""
    
    def __init__(self, vocab_size: int, embedding_dim: int = 128, hidden_dim: int = 256):
        super().__init__()
        
        # –≠–º–±–µ–¥–¥–∏–Ω–≥–∏
        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)
        
        # –≠–Ω–∫–æ–¥–µ—Ä (–æ–ø–∏—Å–∞–Ω–∏–µ ‚Üí —Å–∫—Ä—ã—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ)
        self.encoder_lstm = nn.LSTM(
            input_size=embedding_dim,
            hidden_size=hidden_dim,
            num_layers=2,
            batch_first=True,
            dropout=0.3,
            bidirectional=True
        )
        
        # –î–µ–∫–æ–¥–µ—Ä (—Å–∫—Ä—ã—Ç–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ ‚Üí –∫–æ–¥)
        self.decoder_lstm = nn.LSTM(
            input_size=embedding_dim + hidden_dim * 2,  # —ç–º–±–µ–¥–¥–∏–Ω–≥ + –∫–æ–Ω—Ç–µ–∫—Å—Ç
            hidden_size=hidden_dim,
            num_layers=2,
            batch_first=True,
            dropout=0.3
        )
        
        # –ú–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è
        self.attention = nn.MultiheadAttention(
            embed_dim=hidden_dim * 2,
            num_heads=4,
            dropout=0.3,
            batch_first=True
        )
        
        # –õ–∏–Ω–µ–π–Ω—ã–µ —Å–ª–æ–∏
        self.encoder_proj = nn.Linear(hidden_dim * 2, hidden_dim)
        self.decoder_proj = nn.Linear(hidden_dim, hidden_dim)
        
        # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π
        self.output_layer = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim // 2, vocab_size)
        )
        
        # –°–ª–æ–π –¥–ª—è —Ç–∏–ø–∞ –∏–Ω—Ç–µ–Ω—Ç–∞
        self.intent_embedding = nn.Embedding(10, 32)  # 10 —Ç–∏–ø–æ–≤ –∏–Ω—Ç–µ–Ω—Ç–æ–≤
        
        # –°–ª–æ–π –¥–ª—è —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
        self.complexity_embedding = nn.Embedding(3, 16)  # 3 —É—Ä–æ–≤–Ω—è —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
    
    def forward(self, description, code_input=None, intent_type=None, complexity=None, teacher_forcing_ratio=0.5):
        """
        –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥
        
        Args:
            description: –û–ø–∏—Å–∞–Ω–∏–µ –∫–æ–º–∞–Ω–¥—ã [batch, seq_len]
            code_input: –í—Ö–æ–¥–Ω–æ–π –∫–æ–¥ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è [batch, seq_len]
            intent_type: –¢–∏–ø –Ω–∞–º–µ—Ä–µ–Ω–∏—è
            complexity: –°–ª–æ–∂–Ω–æ—Å—Ç—å –∫–æ–º–∞–Ω–¥—ã
            teacher_forcing_ratio: –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —É—á–∏—Ç–µ–ª—å—Å–∫–æ–≥–æ —Ñ–æ—Ä—Å–∏–Ω–≥–∞
        """
        batch_size = description.size(0)
        seq_len = description.size(1)
        
        # –≠–º–±–µ–¥–¥–∏–Ω–≥ –æ–ø–∏—Å–∞–Ω–∏—è
        desc_embedded = self.embedding(description)
        
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —á–µ—Ä–µ–∑ —ç–Ω–∫–æ–¥–µ—Ä
        encoder_outputs, (hidden, cell) = self.encoder_lstm(desc_embedded)
        encoder_outputs = self.encoder_proj(encoder_outputs)
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–µ–∫–æ–¥–µ—Ä
        decoder_input = torch.full((batch_size, 1), 1, dtype=torch.long, device=description.device)  # <SOS>
        decoder_hidden = hidden
        decoder_cell = cell
        
        # –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        if intent_type is not None:
            intent_emb = self.intent_embedding(intent_type).unsqueeze(1)
        else:
            intent_emb = torch.zeros(batch_size, 1, 32, device=description.device)
        
        if complexity is not None:
            complexity_emb = self.complexity_embedding(complexity).unsqueeze(1)
        else:
            complexity_emb = torch.zeros(batch_size, 1, 16, device=description.device)
        
        # –°–ø–∏—Å–æ–∫ –≤—ã—Ö–æ–¥–æ–≤
        outputs = []
        
        for t in range(seq_len - 1):
            # –≠–º–±–µ–¥–¥–∏–Ω–≥ —Ç–µ–∫—É—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞ –¥–µ–∫–æ–¥–µ—Ä–∞
            decoder_emb = self.embedding(decoder_input)
            
            # –ö–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏—è —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            decoder_emb = torch.cat([
                decoder_emb,
                intent_emb.expand(-1, decoder_emb.size(1), -1),
                complexity_emb.expand(-1, decoder_emb.size(1), -1)
            ], dim=-1)
            
            # –ú–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è
            attn_output, _ = self.attention(
                query=decoder_emb,
                key=encoder_outputs,
                value=encoder_outputs
            )
            
            # –ö–æ–Ω–∫–∞—Ç–µ–Ω–∞—Ü–∏—è —Å –≤—ã—Ö–æ–¥–æ–º –≤–Ω–∏–º–∞–Ω–∏—è
            decoder_input_full = torch.cat([decoder_emb, attn_output], dim=-1)
            
            # –î–µ–∫–æ–¥–µ—Ä
            decoder_output, (decoder_hidden, decoder_cell) = self.decoder_lstm(
                decoder_input_full, (decoder_hidden, decoder_cell)
            )
            
            decoder_output = self.decoder_proj(decoder_output)
            
            # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π
            output = self.output_layer(decoder_output[:, -1, :])
            outputs.append(output.unsqueeze(1))
            
            # –°–ª–µ–¥—É—é—â–∏–π –≤—Ö–æ–¥ –¥–µ–∫–æ–¥–µ—Ä–∞
            if code_input is not None and torch.rand(1).item() < teacher_forcing_ratio:
                # –£—á–∏—Ç–µ–ª—å—Å–∫–∏–π —Ñ–æ—Ä—Å–∏–Ω–≥
                decoder_input = code_input[:, t:t+1]
            else:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
                _, top_idx = output.topk(1)
                decoder_input = top_idx.detach()
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –≤—ã—Ö–æ–¥—ã
        outputs = torch.cat(outputs, dim=1)
        
        return outputs

class NeuralCodeGenerator:
    """–ù–µ–π—Ä–æ—Å–µ—Ç—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞"""
    
    def __init__(self, model_path: str, data_path: str):
        self.model_path = Path(model_path)
        self.data_path = Path(data_path)
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
        self.vocab_size = 5000
        self.embedding_dim = 128
        self.hidden_dim = 256
        self.max_len = 200
        
        # –ú–æ–¥–µ–ª—å –∏ –¥–∞–Ω–Ω—ã–µ
        self.model = None
        self.dataset = None
        self.optimizer = None
        self.criterion = nn.CrossEntropyLoss(ignore_index=0)  # –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º <PAD>
        
        # –¢–∏–ø—ã –∏–Ω—Ç–µ–Ω—Ç–æ–≤
        self.intent_types = {
            'open_program': 0,
            'type_text': 1,
            'create_file': 2,
            'search_web': 3,
            'delete_file': 4,
            'copy_text': 5,
            'paste_text': 6,
            'save_file': 7,
            'system_command': 8,
            'custom': 9
        }
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–ª–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º
        self._load_or_initialize()
        
        # –ë–∞–∑–∞ —à–∞–±–ª–æ–Ω–æ–≤ –¥–ª—è –ø—Ä–æ—Å—Ç—ã—Ö –∫–æ–º–∞–Ω–¥
        self.code_templates = self._load_templates()
    
    def _load_or_initialize(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏–ª–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª—å"""
        if self.model_path.exists():
            print(f"üìÇ –ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ –∫–æ–¥–∞ –∏–∑ {self.model_path}")
            self._load_model()
        else:
            print("üÜï –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ –∫–æ–¥–∞")
            self._initialize_model()
        
        if self.data_path.exists():
            self._load_training_data()
    
    def _initialize_model(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å"""
        self.model = CodeGeneratorModel(
            vocab_size=self.vocab_size,
            embedding_dim=self.embedding_dim,
            hidden_dim=self.hidden_dim
        )
        
        self.optimizer = torch.optim.AdamW(
            self.model.parameters(),
            lr=0.001,
            weight_decay=0.01
        )
    
    def _load_model(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å"""
        try:
            checkpoint = torch.load(self.model_path, map_location='cpu')
            
            self.model = CodeGeneratorModel(
                vocab_size=checkpoint['vocab_size'],
                embedding_dim=checkpoint['embedding_dim'],
                hidden_dim=checkpoint['hidden_dim']
            )
            
            self.model.load_state_dict(checkpoint['model_state'])
            self.optimizer = torch.optim.AdamW(
                self.model.parameters(),
                lr=0.001
            )
            
            if 'intent_types' in checkpoint:
                self.intent_types = checkpoint['intent_types']
            
            print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            self._initialize_model()
    
    def _load_training_data(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        if not self.data_path.exists():
            self.dataset = None
            return
        
        try:
            with open(self.data_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            examples = []
            for item in data:
                example = CodeExample(
                    description=item['description'],
                    code=item['code'],
                    intent_type=item.get('intent_type', 'custom'),
                    complexity=item.get('complexity', 1)
                )
                examples.append(example)
            
            self.dataset = CodeDataset(examples, self.vocab_size, self.max_len)
            print(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(examples)} –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
            self.dataset = None
    
    def _load_templates(self) -> Dict[str, str]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —à–∞–±–ª–æ–Ω—ã –∫–æ–¥–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –∫–æ–º–∞–Ω–¥"""
        templates = {
            'open_program': '''
def execute(program_name=None):
    """–û—Ç–∫—Ä—ã–≤–∞–µ—Ç –ø—Ä–æ–≥—Ä–∞–º–º—É –Ω–∞ –∫–æ–º–ø—å—é—Ç–µ—Ä–µ"""
    import subprocess
    import os
    
    # –°–ª–æ–≤–∞—Ä—å –ø—Ä–æ–≥—Ä–∞–º–º
    programs = {{
        '–±—Ä–∞—É–∑–µ—Ä': 'chrome.exe',
        '–±–ª–æ–∫–Ω–æ—Ç': 'notepad.exe',
        '–∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä': 'calc.exe',
        '–ø—Ä–æ–≤–æ–¥–Ω–∏–∫': 'explorer.exe',
        '—Ç–µ—Ä–º–∏–Ω–∞–ª': 'cmd.exe',
        '–ø–∞–Ω–µ–ª—å —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è': 'control.exe'
    }}
    
    if program_name and program_name in programs:
        target = programs[program_name]
    else:
        # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –ø—Ä–æ–≥—Ä–∞–º–º—É
        for name, exe in programs.items():
            if name in '{program_keyword}':
                target = exe
                break
        else:
            target = 'notepad.exe'
    
    try:
        subprocess.Popen(target)
        return f"–û—Ç–∫—Ä—ã–≤–∞—é {{target}}"
    except Exception as e:
        return f"–û—à–∏–±–∫–∞: {{str(e)}}"
''',
            
            'type_text': '''
def execute(text_to_type=None):
    """–ü–µ—á–∞—Ç–∞–µ—Ç —Ç–µ–∫—Å—Ç –≤ –∞–∫—Ç–∏–≤–Ω–æ–º –æ–∫–Ω–µ"""
    import pyautogui
    import time
    
    if not text_to_type:
        return "–ù–µ —É–∫–∞–∑–∞–Ω —Ç–µ–∫—Å—Ç –¥–ª—è –ø–µ—á–∞—Ç–∏"
    
    # –ñ–¥–µ–º –ø–µ—Ä–µ–¥ –ø–µ—á–∞—Ç—å—é
    time.sleep(1)
    
    try:
        pyautogui.write(text_to_type, interval=0.05)
        return f"–ù–∞–ø–µ—á–∞—Ç–∞–Ω–æ: {{text_to_type}}"
    except Exception as e:
        return f"–û—à–∏–±–∫–∞ –ø–µ—á–∞—Ç–∏: {{str(e)}}"
''',
            
            'create_file': '''
def execute(file_path=None, content=None):
    """–°–æ–∑–¥–∞–µ—Ç —Ñ–∞–π–ª —Å —É–∫–∞–∑–∞–Ω–Ω—ã–º —Å–æ–¥–µ—Ä–∂–∏–º—ã–º"""
    import os
    from pathlib import Path
    
    if not file_path:
        return "–ù–µ —É–∫–∞–∑–∞–Ω –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É"
    
    try:
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        Path(file_path).parent.mkdir(parents=True, exist_ok=True)
        
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(content if content else '')
        
        return f"–§–∞–π–ª —Å–æ–∑–¥–∞–Ω: {{file_path}}"
    except Exception as e:
        return f"–û—à–∏–±–∫–∞: {{str(e)}}"
''',
            
            'search_web': '''
def execute(query=None):
    """–ò—â–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ"""
    import webbrowser
    import urllib.parse
    
    if not query:
        return "–ù–µ —É–∫–∞–∑–∞–Ω –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞"
    
    # –ö–æ–¥–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å
    encoded_query = urllib.parse.quote(query)
    
    # –û—Ç–∫—Ä—ã–≤–∞–µ–º –≤ –±—Ä–∞—É–∑–µ—Ä–µ
    search_url = f"https://www.google.com/search?q={{encoded_query}}"
    webbrowser.open(search_url)
    
    return f"–ò—â—É –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ: {{query}}"
'''
        }
        
        return templates
    
    def save_model(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–æ–¥–µ–ª—å"""
        checkpoint = {
            'model_state': self.model.state_dict(),
            'vocab_size': self.vocab_size,
            'embedding_dim': self.embedding_dim,
            'hidden_dim': self.hidden_dim,
            'intent_types': self.intent_types
        }
        
        torch.save(checkpoint, self.model_path)
        print(f"üíæ –ú–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ –∫–æ–¥–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")
    
    def generate(self, description: str, intent_type: str = 'custom', safe_mode: bool = True) -> str:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–æ–¥ –ø–æ –æ–ø–∏—Å–∞–Ω–∏—é –∫–æ–º–∞–Ω–¥—ã
        
        Args:
            description: –û–ø–∏—Å–∞–Ω–∏–µ –∫–æ–º–∞–Ω–¥—ã
            intent_type: –¢–∏–ø –Ω–∞–º–µ—Ä–µ–Ω–∏—è
            safe_mode: –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π —Ä–µ–∂–∏–º (–∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —à–∞–±–ª–æ–Ω—ã –µ—Å–ª–∏ –Ω–µ–π—Ä–æ—Å–µ—Ç—å –Ω–µ —É–≤–µ—Ä–µ–Ω–∞)
            
        Returns:
            –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π Python –∫–æ–¥
        """
        print(f"‚öôÔ∏è –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–¥–∞ –¥–ª—è: '{description}'")
        
        # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º —à–∞–±–ª–æ–Ω—ã
        if self.model is None or self.dataset is None:
            print("‚ö†Ô∏è –ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É—é —à–∞–±–ª–æ–Ω—ã")
            return self._generate_from_template(description, intent_type)
        
        try:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–ª–æ–∂–Ω–æ—Å—Ç—å
            complexity = self._estimate_complexity(description)
            
            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω–¥–µ–∫—Å —Ç–∏–ø–∞ –∏–Ω—Ç–µ–Ω—Ç–∞
            intent_idx = self.intent_types.get(intent_type, 9)  # 9 = custom
            
            # –ö–æ–¥–∏—Ä—É–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ
            desc_encoded = self.dataset.encode_text(description, add_special=True)
            desc_tensor = torch.tensor([desc_encoded], dtype=torch.long)
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è
            self.model.eval()
            with torch.no_grad():
                # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥
                outputs = self.model(
                    desc_tensor,
                    intent_type=torch.tensor([intent_idx], dtype=torch.long),
                    complexity=torch.tensor([complexity], dtype=torch.long),
                    teacher_forcing_ratio=0.0
                )
                
                # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –≤—ã—Ö–æ–¥
                _, predicted = torch.max(outputs, dim=2)
                predicted = predicted.squeeze(0).tolist()
                
                # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –≤ —Ç–µ–∫—Å—Ç
                generated_code = self._decode_indices(predicted)
                
                # –û—á–∏—â–∞–µ–º –∫–æ–¥
                generated_code = self._clean_generated_code(generated_code)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ
                is_valid, error = self._validate_generated_code(generated_code)
                
                if not is_valid and safe_mode:
                    print(f"‚ö†Ô∏è –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–¥ –Ω–µ–≤–∞–ª–∏–¥–µ–Ω: {error}")
                    print("üîÑ –ò—Å–ø–æ–ª—å–∑—É—é —à–∞–±–ª–æ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥")
                    return self._generate_from_template(description, intent_type)
                
                return generated_code
                
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
            return self._generate_from_template(description, intent_type)
    
    def _decode_indices(self, indices: List[int]) -> str:
        """–î–µ–∫–æ–¥–∏—Ä—É–µ—Ç –∏–Ω–¥–µ–∫—Å—ã –æ–±—Ä–∞—Ç–Ω–æ –≤ —Ç–µ–∫—Å—Ç"""
        if not hasattr(self, 'dataset') or self.dataset is None:
            return ""
        
        # –£–±–∏—Ä–∞–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
        decoded = []
        for idx in indices:
            if idx == 0 or idx == 1 or idx == 2:  # <PAD>, <SOS>, <EOS>
                continue
            
            if idx in self.dataset.idx_to_char:
                decoded.append(self.dataset.idx_to_char[idx])
            else:
                decoded.append('?')
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤ —Å—Ç—Ä–æ–∫—É
        return ''.join(decoded).strip()
    
    def _clean_generated_code(self, code: str) -> str:
        """–û—á–∏—â–∞–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–¥"""
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        code = re.sub(r'\n\s*\n', '\n\n', code)
        
        # –£–±–∏—Ä–∞–µ–º –Ω–µ–ø–æ–ª–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏ –≤ –∫–æ–Ω—Ü–µ
        lines = code.strip().split('\n')
        cleaned_lines = []
        
        for line in lines:
            line = line.rstrip()
            if line:  # –ù–µ –ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞
                cleaned_lines.append(line)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –µ—Å—Ç—å def
        if not any('def ' in line for line in cleaned_lines):
            # –î–æ–±–∞–≤–ª—è–µ–º –±–∞–∑–æ–≤—É—é —Ñ—É–Ω–∫—Ü–∏—é
            cleaned_lines.insert(0, 'def execute():')
            cleaned_lines.insert(1, '    """–í—ã–ø–æ–ª–Ω—è–µ—Ç –∫–æ–º–∞–Ω–¥—É"""')
            cleaned_lines.insert(2, '    return "–ö–æ–º–∞–Ω–¥–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞"')
        
        return '\n'.join(cleaned_lines)
    
    def _validate_generated_code(self, code: str) -> Tuple[bool, str]:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–æ–¥–∞"""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–∏–Ω—Ç–∞–∫—Å–∏—Å
            ast.parse(code)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –æ–ø–∞—Å–Ω—ã—Ö –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–π
            dangerous_patterns = [
                (r'__import__\s*\(', '__import__'),
                (r'eval\s*\(', 'eval'),
                (r'exec\s*\(', 'exec'),
                (r'os\.system\s*\(', 'os.system'),
                (r'subprocess\.(?:call|run|Popen)\(.*shell\s*=\s*True', 'shell=True')
            ]
            
            for pattern, name in dangerous_patterns:
                if re.search(pattern, code, re.IGNORECASE):
                    return False, f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –æ–ø–∞—Å–Ω–∞—è –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è: {name}"
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ execute
            if not re.search(r'def\s+\w+\(', code):
                return False, "–ù–µ—Ç —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è"
            
            return True, ""
            
        except SyntaxError as e:
            return False, f"–°–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}"
        except Exception as e:
            return False, f"–û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {e}"
    
    def _generate_from_template(self, description: str, intent_type: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–æ–¥ –∏–∑ —à–∞–±–ª–æ–Ω–∞"""
        template = self.code_templates.get(intent_type, self.code_templates['custom'])
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –æ–ø–∏—Å–∞–Ω–∏—è
        program_keywords = ['–±—Ä–∞—É–∑–µ—Ä', '–±–ª–æ–∫–Ω–æ—Ç', '–∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä', '–ø—Ä–æ–≤–æ–¥–Ω–∏–∫', '—Ç–µ—Ä–º–∏–Ω–∞–ª', '–ø–∞–Ω–µ–ª—å']
        found_keywords = [kw for kw in program_keywords if kw in description.lower()]
        
        if found_keywords:
            program_keyword = found_keywords[0]
        else:
            program_keyword = '–±–ª–æ–∫–Ω–æ—Ç'
        
        # –ó–∞–º–µ–Ω—è–µ–º –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä—ã
        code = template.format(program_keyword=program_keyword)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–º–ø–æ—Ä—Ç—ã –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç
        if 'import' not in code:
            code = "import os\nimport subprocess\n" + code
        
        return code
    
    def _estimate_complexity(self, description: str) -> int:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç—å –∫–æ–º–∞–Ω–¥—ã"""
        desc_lower = description.lower()
        
        # –ü—Ä–æ—Å—Ç—ã–µ –∫–æ–º–∞–Ω–¥—ã
        simple_keywords = ['–æ—Ç–∫—Ä–æ–π', '–∑–∞–∫—Ä–æ–π', '–ø—Ä–∏–≤–µ—Ç', '–ø–æ–∫–∞', '–≤—Ä–µ–º—è']
        if any(kw in desc_lower for kw in simple_keywords):
            return 1
        
        # –°—Ä–µ–¥–Ω–∏–µ –∫–æ–º–∞–Ω–¥—ã
        medium_keywords = ['—Å–æ–∑–¥–∞–π', '—É–¥–∞–ª–∏', '–Ω–∞–π–¥–∏', '–ø–æ–∏—â–∏', '—Å–∫–æ–ø–∏—Ä—É–π', '–≤—Å—Ç–∞–≤—å']
        if any(kw in desc_lower for kw in medium_keywords):
            return 2
        
        # –°–ª–æ–∂–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã
        complex_keywords = ['–∑–∞–ø—É—Å—Ç–∏ –ø—Ä–æ—Ü–µ—Å—Å', '–Ω–∞—Å—Ç—Ä–æ–π', '–∞–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä—É–π', '—Å–¥–µ–ª–∞–π —Ä–µ–∑–µ—Ä–≤–Ω—É—é –∫–æ–ø–∏—é']
        if any(kw in desc_lower for kw in complex_keywords):
            return 3
        
        return 2  # –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é —Å—Ä–µ–¥–Ω—è—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å
    
    def train_on_example(self, description: str, code: str, intent_type: str = 'custom'):
        """
        –û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å –Ω–∞ –Ω–æ–≤–æ–º –ø—Ä–∏–º–µ—Ä–µ
        
        Args:
            description: –û–ø–∏—Å–∞–Ω–∏–µ –∫–æ–º–∞–Ω–¥—ã
            code: Python –∫–æ–¥
            intent_type: –¢–∏–ø –Ω–∞–º–µ—Ä–µ–Ω–∏—è
        """
        if self.dataset is None:
            # –°–æ–∑–¥–∞–µ–º –ø–µ—Ä–≤—ã–π –ø—Ä–∏–º–µ—Ä
            example = CodeExample(
                description=description,
                code=code,
                intent_type=intent_type,
                complexity=self._estimate_complexity(description)
            )
            self.dataset = CodeDataset([example], self.vocab_size, self.max_len)
        else:
            # –î–æ–±–∞–≤–ª—è–µ–º –∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º
            examples = self.dataset.examples.copy()
            examples.append(CodeExample(
                description=description,
                code=code,
                intent_type=intent_type,
                complexity=self._estimate_complexity(description)
            ))
            self.dataset = CodeDataset(examples, self.vocab_size, self.max_len)
        
        # –û–±—É—á–∞–µ–º –Ω–∞ –æ–¥–Ω–æ–º –ø—Ä–∏–º–µ—Ä–µ
        self._train_one_epoch()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ
        self._save_training_example(description, code, intent_type)
        
        print(f"‚úÖ –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –Ω–∞ –ø—Ä–∏–º–µ—Ä–µ: '{description[:50]}...'")
    
    def _train_one_epoch(self):
        """–û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å –Ω–∞ –æ–¥–Ω–æ–π —ç–ø–æ—Ö–µ"""
        if self.dataset is None or len(self.dataset) == 0:
            return
        
        dataloader = DataLoader(
            self.dataset,
            batch_size=2,
            shuffle=True
        )
        
        self.model.train()
        total_loss = 0
        
        for batch in dataloader:
            self.optimizer.zero_grad()
            
            description = batch['description']
            code = batch['code']
            intent_type = batch.get('intent_type', None)
            complexity = batch.get('complexity', None)
            
            # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥
            outputs = self.model(
                description,
                code_input=code[:, :-1],  # –í—Ö–æ–¥ –¥–ª—è —É—á–∏—Ç–µ–ª—å—Å–∫–æ–≥–æ —Ñ–æ—Ä—Å–∏–Ω–≥–∞
                intent_type=intent_type,
                complexity=complexity,
                teacher_forcing_ratio=0.7
            )
            
            # –í—ã—á–∏—Å–ª—è–µ–º –ø–æ—Ç–µ—Ä—é
            loss = self.criterion(
                outputs.reshape(-1, outputs.size(-1)),
                code[:, 1:].reshape(-1)  # –°–¥–≤–∏–Ω—É—Ç–∞—è —Ü–µ–ª—å
            )
            
            # –û–±—Ä–∞—Ç–Ω—ã–π –ø—Ä–æ—Ö–æ–¥
            loss.backward()
            self.optimizer.step()
            
            total_loss += loss.item()
        
        avg_loss = total_loss / len(dataloader) if len(dataloader) > 0 else 0
        print(f"  –û–±—É—á–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞: Loss = {avg_loss:.4f}")
    
    def _save_training_example(self, description: str, code: str, intent_type: str):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø—Ä–∏–º–µ—Ä –æ–±—É—á–µ–Ω–∏—è"""
        data = []
        if self.data_path.exists():
            try:
                with open(self.data_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
            except:
                pass
        
        data.append({
            'description': description,
            'code': code,
            'intent_type': intent_type,
            'complexity': self._estimate_complexity(description),
            'timestamp': time.time() if 'time' in globals() else 0
        })
        
        with open(self.data_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"üíæ –ü—Ä–∏–º–µ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {self.data_path}")

# –£—Ç–∏–ª–∏—Ç–∞ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
def test_code_generator():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∫–æ–¥–∞"""
    print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ –∫–æ–¥–∞...")
    
    import tempfile
    temp_dir = tempfile.mkdtemp()
    model_path = os.path.join(temp_dir, "code_generator.pt")
    data_path = os.path.join(temp_dir, "code_data.json")
    
    # –°–æ–∑–¥–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä
    generator = NeuralCodeGenerator(model_path, data_path)
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ –ø—Ä–∏–º–µ—Ä—ã
    test_cases = [
        {
            "description": "–æ—Ç–∫—Ä–æ–π –±—Ä–∞—É–∑–µ—Ä",
            "intent_type": "open_program"
        },
        {
            "description": "–Ω–∞–ø–µ—á–∞—Ç–∞–π –ø—Ä–∏–≤–µ—Ç –º–∏—Ä",
            "intent_type": "type_text"
        },
        {
            "description": "—Å–æ–∑–¥–∞–π —Ñ–∞–π–ª report.txt —Å —Ç–µ–∫—Å—Ç–æ–º –æ—Ç—á–µ—Ç",
            "intent_type": "create_file"
        }
    ]
    
    for test in test_cases:
        print(f"\nüìù –¢–µ—Å—Ç: '{test['description']}'")
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∫–æ–¥
        generated_code = generator.generate(
            description=test['description'],
            intent_type=test['intent_type']
        )
        
        print(f"üìÑ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–¥:\n{generated_code[:200]}...")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å
        is_valid, error = generator._validate_generated_code(generated_code)
        if is_valid:
            print("‚úÖ –ö–æ–¥ –≤–∞–ª–∏–¥–µ–Ω")
        else:
            print(f"‚ùå –ö–æ–¥ –Ω–µ–≤–∞–ª–∏–¥–µ–Ω: {error}")
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –æ–±—É—á–µ–Ω–∏–µ
    print("\nüéì –¢–µ—Å—Ç–∏—Ä—É–µ–º –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –Ω–æ–≤–æ–º –ø—Ä–∏–º–µ—Ä–µ...")
    
    training_description = "–∑–∞–∫—Ä–æ–π –≤—Å–µ –æ–∫–Ω–∞"
    training_code = '''
def execute():
    """–ó–∞–∫—Ä—ã–≤–∞–µ—Ç –≤—Å–µ –æ–∫–Ω–∞"""
    import pyautogui
    import time
    
    # –ó–∞–∫—Ä—ã–≤–∞–µ–º –∞–∫—Ç–∏–≤–Ω–æ–µ –æ–∫–Ω–æ
    pyautogui.hotkey('alt', 'f4')
    time.sleep(0.5)
    
    return "–û–∫–Ω–∞ –∑–∞–∫—Ä—ã—Ç—ã"
'''
    
    generator.train_on_example(training_description, training_code, "custom")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
    generator.save_model()
    
    # –û—á–∏—Å—Ç–∫–∞
    import shutil
    shutil.rmtree(temp_dir)
    print("\nüßπ –¢–µ—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã —É–¥–∞–ª–µ–Ω—ã")

if __name__ == "__main__":
    import time
    test_code_generator()