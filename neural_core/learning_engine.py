"""
–ù–µ–π—Ä–æ—Å–µ—Ç—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–æ–≤—ã–º –∫–æ–º–∞–Ω–¥–∞–º
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import json
import os
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional
import numpy as np
from dataclasses import dataclass
from collections import defaultdict

@dataclass
class TrainingExample:
    """–ü—Ä–∏–º–µ—Ä –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ—Å–µ—Ç–∏"""
    text: str
    intent_label: str
    explanation: str
    examples: List[str]
    embedding: Optional[np.ndarray] = None

class CommandDataset(Dataset):
    """–î–∞—Ç–∞—Å–µ—Ç –∫–æ–º–∞–Ω–¥ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ—Å–µ—Ç–∏"""
    
    def __init__(self, examples: List[TrainingExample], vocab_size: int = 10000):
        self.examples = examples
        self.vocab_size = vocab_size
        self.word_to_idx = {}
        self.idx_to_word = {}
        self._build_vocab()
    
    def _build_vocab(self):
        """–°—Ç—Ä–æ–∏—Ç —Å–ª–æ–≤–∞—Ä—å –∏–∑ –≤—Å–µ—Ö —Ç–µ–∫—Å—Ç–æ–≤"""
        word_counts = defaultdict(int)
        
        for example in self.examples:
            for word in example.text.lower().split():
                word_counts[word] += 1
        
        # –ë–µ—Ä–µ–º —Å–∞–º—ã–µ —á–∞—Å—Ç—ã–µ —Å–ª–æ–≤–∞
        sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)
        vocab_words = [word for word, _ in sorted_words[:self.vocab_size]]
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
        self.word_to_idx = {'<PAD>': 0, '<UNK>': 1}
        self.idx_to_word = {0: '<PAD>', 1: '<UNK>'}
        
        for idx, word in enumerate(vocab_words, start=2):
            self.word_to_idx[word] = idx
            self.idx_to_word[idx] = word
    
    def text_to_indices(self, text: str, max_len: int = 50) -> List[int]:
        """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Ç–µ–∫—Å—Ç –≤ –∏–Ω–¥–µ–∫—Å—ã"""
        words = text.lower().split()[:max_len]
        indices = [self.word_to_idx.get(word, 1) for word in words]
        
        # –ü–∞–¥–¥–∏–Ω–≥
        if len(indices) < max_len:
            indices += [0] * (max_len - len(indices))
        
        return indices
    
    def __len__(self):
        return len(self.examples)
    
    def __getitem__(self, idx):
        example = self.examples[idx]
        text_indices = self.text_to_indices(example.text)
        
        # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç–∫—É –∏–Ω—Ç–µ–Ω—Ç–∞ –∫–∞–∫ –∏–Ω–¥–µ–∫—Å
        intent_idx = self._get_intent_index(example.intent_label)
        
        return {
            'text': torch.tensor(text_indices, dtype=torch.long),
            'intent': torch.tensor(intent_idx, dtype=torch.long),
            'text_str': example.text,
            'intent_label': example.intent_label
        }
    
    def _get_intent_index(self, intent_label: str) -> int:
        """–ü–æ–ª—É—á–∞–µ—Ç –∏–Ω–¥–µ–∫—Å –Ω–∞–º–µ—Ä–µ–Ω–∏—è"""
        # –ü—Ä–æ—Å—Ç–∞—è —Ö—ç—à-—Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∏–Ω—Ç–µ–Ω—Ç–æ–≤
        return hash(intent_label) % 1000

class IntentClassifier(nn.Module):
    """–ù–µ–π—Ä–æ—Å–µ—Ç—å –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω–∞–º–µ—Ä–µ–Ω–∏–π"""
    
    def __init__(self, vocab_size: int, embedding_dim: int = 128, hidden_dim: int = 256, num_intents: int = 50):
        super().__init__()
        
        # –°–ª–æ–π —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)
        
        # LSTM –¥–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
        self.lstm = nn.LSTM(
            input_size=embedding_dim,
            hidden_size=hidden_dim,
            num_layers=2,
            batch_first=True,
            dropout=0.3,
            bidirectional=True
        )
        
        # –í–Ω–∏–º–∞–Ω–∏–µ
        self.attention = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, 1)
        )
        
        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim // 2, num_intents)
        )
        
        # –°–ª–æ–π –¥–ª—è –Ω–æ–≤—ã—Ö –∏–Ω—Ç–µ–Ω—Ç–æ–≤
        self.new_intent_projection = nn.Linear(hidden_dim * 2, 64)
        
    def forward(self, text_ids, attention_mask=None):
        # –≠–º–±–µ–¥–¥–∏–Ω–≥–∏
        embedded = self.embedding(text_ids)
        
        # LSTM
        lstm_out, (hidden, cell) = self.lstm(embedded)
        
        # –í–Ω–∏–º–∞–Ω–∏–µ
        attention_weights = F.softmax(self.attention(lstm_out).squeeze(-1), dim=1)
        
        # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è —Å—É–º–º–∞
        context_vector = torch.bmm(attention_weights.unsqueeze(1), lstm_out).squeeze(1)
        
        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
        logits = self.classifier(context_vector)
        
        # –≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –Ω–æ–≤—ã—Ö –∏–Ω—Ç–µ–Ω—Ç–æ–≤
        new_intent_embedding = self.new_intent_projection(context_vector)
        
        return logits, new_intent_embedding

class SimilarityFinder(nn.Module):
    """–ù–µ–π—Ä–æ—Å–µ—Ç—å –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ—Ö–æ–∂–∏—Ö –∫–æ–º–∞–Ω–¥"""
    
    def __init__(self, embedding_dim: int = 64):
        super().__init__()
        self.embedding_dim = embedding_dim
        
        # –ü—Ä–æ–µ–∫—Ü–∏—è –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        self.projection = nn.Sequential(
            nn.Linear(embedding_dim * 2, embedding_dim),
            nn.ReLU(),
            nn.Linear(embedding_dim, embedding_dim)
        )
        
    def forward(self, x1, x2):
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ö–æ–∂–µ—Å—Ç—å –º–µ–∂–¥—É –¥–≤—É–º—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏"""
        # –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
        cos_sim = F.cosine_similarity(x1, x2, dim=-1)
        return cos_sim

class NeuralLearningEngine:
    """–î–≤–∏–∂–æ–∫ –æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ –Ω–æ–≤—ã–º –∫–æ–º–∞–Ω–¥–∞–º"""
    
    def __init__(self, model_path: str, data_path: str):
        self.model_path = Path(model_path)
        self.data_path = Path(data_path)
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        self.model_path.parent.mkdir(parents=True, exist_ok=True)
        self.data_path.parent.mkdir(parents=True, exist_ok=True)
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
        self.vocab_size = 10000
        self.embedding_dim = 128
        self.hidden_dim = 256
        self.max_seq_len = 50
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π
        self.intent_classifier = None
        self.similarity_finder = None
        self.dataset = None
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
        self.optimizer = None
        self.criterion = nn.CrossEntropyLoss()
        
        # –î–∞–Ω–Ω—ã–µ
        self.known_intents = set()
        self.intent_examples = defaultdict(list)
        self.intent_embeddings = {}
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –µ—Å–ª–∏ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        self._load_or_initialize()
    
    def _load_or_initialize(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –º–æ–¥–µ–ª—å –∏–ª–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –Ω–æ–≤—É—é"""
        if self.model_path.exists():
            print(f"üìÇ –ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å –∏–∑ {self.model_path}")
            self._load_model()
        else:
            print("üÜï –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å")
            self._initialize_model()
        
        if self.data_path.exists():
            self._load_data()
    
    def _initialize_model(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –Ω–æ–≤—ã–µ –º–æ–¥–µ–ª–∏"""
        self.intent_classifier = IntentClassifier(
            vocab_size=self.vocab_size,
            embedding_dim=self.embedding_dim,
            hidden_dim=self.hidden_dim,
            num_intents=50  # –ù–∞—á–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–Ω—Ç–µ–Ω—Ç–æ–≤
        )
        
        self.similarity_finder = SimilarityFinder(embedding_dim=64)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
        self.optimizer = optim.AdamW(
            list(self.intent_classifier.parameters()) + 
            list(self.similarity_finder.parameters()),
            lr=0.001
        )
    
    def _load_model(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å"""
        try:
            checkpoint = torch.load(self.model_path, map_location='cpu')
            
            self.intent_classifier = IntentClassifier(
                vocab_size=checkpoint['vocab_size'],
                embedding_dim=checkpoint['embedding_dim'],
                hidden_dim=checkpoint['hidden_dim'],
                num_intents=checkpoint['num_intents']
            )
            
            self.intent_classifier.load_state_dict(checkpoint['intent_classifier_state'])
            
            self.similarity_finder = SimilarityFinder(
                embedding_dim=checkpoint['similarity_embedding_dim']
            )
            
            if 'similarity_finder_state' in checkpoint:
                self.similarity_finder.load_state_dict(checkpoint['similarity_finder_state'])
            
            self.optimizer = optim.AdamW(
                list(self.intent_classifier.parameters()) + 
                list(self.similarity_finder.parameters()),
                lr=0.001
            )
            
            self.known_intents = set(checkpoint['known_intents'])
            self.vocab_size = checkpoint['vocab_size']
            
            print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –ó–Ω–∞—é {len(self.known_intents)} –Ω–∞–º–µ—Ä–µ–Ω–∏–π")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            self._initialize_model()
    
    def _load_data(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ñ–∞–π–ª–∞"""
        try:
            with open(self.data_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            examples = []
            for item in data:
                example = TrainingExample(
                    text=item['text'],
                    intent_label=item['intent_label'],
                    explanation=item['explanation'],
                    examples=item['examples']
                )
                examples.append(example)
                self.known_intents.add(item['intent_label'])
            
            # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
            self.dataset = CommandDataset(examples, self.vocab_size)
            
            print(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(examples)} –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è {len(self.known_intents)} –Ω–∞–º–µ—Ä–µ–Ω–∏–π")
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
            self.dataset = None
    
    def save_model(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–æ–¥–µ–ª—å"""
        checkpoint = {
            'intent_classifier_state': self.intent_classifier.state_dict(),
            'similarity_finder_state': self.similarity_finder.state_dict() if self.similarity_finder else None,
            'vocab_size': self.vocab_size,
            'embedding_dim': self.embedding_dim,
            'hidden_dim': self.hidden_dim,
            'similarity_embedding_dim': 64,
            'num_intents': len(self.known_intents) + 10,  # +10 –¥–ª—è –∑–∞–ø–∞—Å–∞
            'known_intents': list(self.known_intents)
        }
        
        torch.save(checkpoint, self.model_path)
        print(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {self.model_path}")
    
    def train_on_example(self, text: str, explanation: str, examples: List[str], generated_code: str) -> bool:
        """
        –û–±—É—á–∞–µ—Ç –Ω–µ–π—Ä–æ—Å–µ—Ç—å –Ω–∞ –Ω–æ–≤–æ–º –ø—Ä–∏–º–µ—Ä–µ –∫–æ–º–∞–Ω–¥—ã
        
        Args:
            text: –¢–µ–∫—Å—Ç –∫–æ–º–∞–Ω–¥—ã
            explanation: –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –∫–æ–º–∞–Ω–¥—ã
            examples: –ü—Ä–∏–º–µ—Ä—ã –ø–æ—Ö–æ–∂–∏—Ö –∫–æ–º–∞–Ω–¥
            generated_code: –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–¥ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
            
        Returns:
            True –µ—Å–ª–∏ –æ–±—É—á–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ
        """
        try:
            print(f"üéì –û–±—É—á–∞—é –Ω–µ–π—Ä–æ—Å–µ—Ç—å –Ω–∞ –∫–æ–º–∞–Ω–¥–µ: '{text}'")
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –º–µ—Ç–∫—É –¥–ª—è –Ω–æ–≤–æ–≥–æ –∏–Ω—Ç–µ–Ω—Ç–∞
            intent_label = self._generate_intent_label(text, explanation)
            
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–º–µ—Ä –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            training_example = TrainingExample(
                text=text,
                intent_label=intent_label,
                explanation=explanation,
                examples=examples
            )
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∏–∑–≤–µ—Å—Ç–Ω—ã–µ –∏–Ω—Ç–µ–Ω—Ç—ã
            self.known_intents.add(intent_label)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –¥–∞—Ç–∞—Å–µ—Ç
            if self.dataset is None:
                # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç
                self.dataset = CommandDataset([training_example], self.vocab_size)
            else:
                # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –¥–∞—Ç–∞—Å–µ—Ç
                # –î–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã –ø–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º —Å –Ω–æ–≤—ã–º –ø—Ä–∏–º–µ—Ä–æ–º
                current_examples = self.dataset.examples.copy()
                current_examples.append(training_example)
                self.dataset = CommandDataset(current_examples, self.vocab_size)
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            dataloader = DataLoader(
                self.dataset,
                batch_size=4,
                shuffle=True
            )
            
            # –û–±—É—á–µ–Ω–∏–µ
            self._train_epoch(dataloader, num_epochs=5)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
            self.save_model()
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ
            self._save_training_example(training_example, generated_code)
            
            # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –Ω–æ–≤–æ–π –∫–æ–º–∞–Ω–¥—ã
            self._create_command_embedding(training_example)
            
            print(f"‚úÖ –ù–µ–π—Ä–æ—Å–µ—Ç—å –æ–±—É—á–µ–Ω–∞ –Ω–∞ –∫–æ–º–∞–Ω–¥–µ '{text}'")
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è: {e}")
            return False
    
    def _generate_intent_label(self, text: str, explanation: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—É—é –º–µ—Ç–∫—É –¥–ª—è –Ω–∞–º–µ—Ä–µ–Ω–∏—è"""
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ö—ç—à –æ—Ç —Ç–µ–∫—Å—Ç–∞ –∏ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è
        import hashlib
        combined = text + "||" + explanation[:100]
        hash_obj = hashlib.sha256(combined.encode())
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        if any(word in explanation.lower() for word in ['–æ—Ç–∫—Ä—ã—Ç—å', '–∑–∞–ø—É—Å—Ç–∏—Ç—å']):
            prefix = "open_"
        elif any(word in explanation.lower() for word in ['—Å–æ–∑–¥–∞—Ç—å', '—Å–¥–µ–ª–∞—Ç—å']):
            prefix = "create_"
        elif any(word in explanation.lower() for word in ['–Ω–∞–ø–µ—á–∞—Ç–∞—Ç—å', '–Ω–∞–ø–∏—Å–∞—Ç—å']):
            prefix = "type_"
        else:
            prefix = "cmd_"
        
        return prefix + hash_obj.hexdigest()[:8]
    
    def _train_epoch(self, dataloader: DataLoader, num_epochs: int = 5):
        """–û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å –Ω–∞ –æ–¥–Ω–æ–π —ç–ø–æ—Ö–µ"""
        self.intent_classifier.train()
        
        for epoch in range(num_epochs):
            total_loss = 0
            correct = 0
            total = 0
            
            for batch in dataloader:
                self.optimizer.zero_grad()
                
                # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ
                text_ids = batch['text']
                intent_labels = batch['intent']
                
                # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥
                logits, embeddings = self.intent_classifier(text_ids)
                
                # –í—ã—á–∏—Å–ª—è–µ–º –ø–æ—Ç–µ—Ä—é
                loss = self.criterion(logits, intent_labels)
                
                # –û–±—Ä–∞—Ç–Ω—ã–π –ø—Ä–æ—Ö–æ–¥
                loss.backward()
                self.optimizer.step()
                
                # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
                total_loss += loss.item()
                _, predicted = torch.max(logits, 1)
                total += intent_labels.size(0)
                correct += (predicted == intent_labels).sum().item()
            
            accuracy = 100 * correct / total if total > 0 else 0
            avg_loss = total_loss / len(dataloader) if len(dataloader) > 0 else 0
            
            print(f"  –≠–ø–æ—Ö–∞ {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.1f}%")
    
    def _save_training_example(self, example: TrainingExample, generated_code: str):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø—Ä–∏–º–µ—Ä –æ–±—É—á–µ–Ω–∏—è"""
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ
        data = []
        if self.data_path.exists():
            try:
                with open(self.data_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
            except:
                pass
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–π –ø—Ä–∏–º–µ—Ä
        data.append({
            'text': example.text,
            'intent_label': example.intent_label,
            'explanation': example.explanation,
            'examples': example.examples,
            'generated_code': generated_code,
            'timestamp': time.time() if 'time' in globals() else 0
        })
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º
        with open(self.data_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    
    def _create_command_embedding(self, example: TrainingExample):
        """–°–æ–∑–¥–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –∫–æ–º–∞–Ω–¥—ã"""
        try:
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–µ–∫—Å—Ç –≤ –∏–Ω–¥–µ–∫—Å—ã
            if self.dataset:
                indices = self.dataset.text_to_indices(example.text)
                text_tensor = torch.tensor([indices], dtype=torch.long)
                
                # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥
                with torch.no_grad():
                    _, embedding = self.intent_classifier(text_tensor)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥
                self.intent_embeddings[example.intent_label] = embedding.numpy()
                
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {e}")
    
    def find_similar_command(self, text: str, threshold: float = 0.7) -> Optional[Dict[str, Any]]:
        """
        –ò—â–µ—Ç –ø–æ—Ö–æ–∂–∏–µ –∫–æ–º–∞–Ω–¥—ã –≤ –ø–∞–º—è—Ç–∏
        
        Args:
            text: –¢–µ–∫—Å—Ç –∫–æ–º–∞–Ω–¥—ã
            threshold: –ü–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏
            
        Returns:
            –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–æ—Ö–æ–∂–µ–π –∫–æ–º–∞–Ω–¥–µ –∏–ª–∏ None
        """
        if not self.intent_embeddings:
            return None
        
        try:
            # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –≤—Ö–æ–¥–Ω–æ–π –∫–æ–º–∞–Ω–¥—ã
            if self.dataset:
                indices = self.dataset.text_to_indices(text)
                text_tensor = torch.tensor([indices], dtype=torch.long)
                
                with torch.no_grad():
                    _, query_embedding = self.intent_classifier(text_tensor)
                
                query_embedding = query_embedding.numpy()
                
                # –ò—â–µ–º –ø–æ—Ö–æ–∂–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
                best_match = None
                best_similarity = 0
                
                for intent_label, stored_embedding in self.intent_embeddings.items():
                    # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
                    cos_sim = np.dot(query_embedding.flatten(), stored_embedding.flatten())
                    cos_sim /= (np.linalg.norm(query_embedding) * np.linalg.norm(stored_embedding) + 1e-8)
                    
                    if cos_sim > best_similarity:
                        best_similarity = cos_sim
                        best_match = intent_label
                
                if best_match and best_similarity > threshold:
                    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–æ–º–∞–Ω–¥–µ
                    if self.data_path.exists():
                        with open(self.data_path, 'r', encoding='utf-8') as f:
                            data = json.load(f)
                        
                        for item in data:
                            if item['intent_label'] == best_match:
                                return {
                                    'command': item['text'],
                                    'intent_label': best_match,
                                    'similarity': best_similarity,
                                    'explanation': item['explanation'],
                                    'generated_code': item.get('generated_code', '')
                                }
            
            return None
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –ø–æ—Ö–æ–∂–∏—Ö –∫–æ–º–∞–Ω–¥: {e}")
            return None
    
    def predict_intent(self, text: str) -> Tuple[str, float]:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞–º–µ—Ä–µ–Ω–∏–µ –¥–ª—è —Ç–µ–∫—Å—Ç–∞
        
        Returns:
            –ú–µ—Ç–∫–∞ –Ω–∞–º–µ—Ä–µ–Ω–∏—è –∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
        """
        try:
            if not self.dataset:
                return "unknown", 0.0
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ç–µ–∫—Å—Ç
            indices = self.dataset.text_to_indices(text)
            text_tensor = torch.tensor([indices], dtype=torch.long)
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            with torch.no_grad():
                self.intent_classifier.eval()
                logits, _ = self.intent_classifier(text_tensor)
                probabilities = F.softmax(logits, dim=1)
                
                # –ü–æ–ª—É—á–∞–µ–º —Å–∞–º–æ–µ –≤–µ—Ä–æ—è—Ç–Ω–æ–µ –Ω–∞–º–µ—Ä–µ–Ω–∏–µ
                confidence, predicted_idx = torch.max(probabilities, 1)
                
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∏–Ω–¥–µ–∫—Å –≤ –º–µ—Ç–∫—É (–Ω—É–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –º–∞–ø–ø–∏–Ω–≥)
                # –ü–æ–∫–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∏–Ω–¥–µ–∫—Å
                intent_label = f"intent_{predicted_idx.item()}"
                
                return intent_label, confidence.item()
                
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {e}")
            return "error", 0.0
    
    def retrain_on_all_data(self):
        """–ü–µ—Ä–µ–æ–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö"""
        if not self.data_path.exists():
            print("üì≠ –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è")
            return False
        
        try:
            print("üîÑ –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö...")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ
            with open(self.data_path, 'r', encoding='utf-8') as f:
                all_data = json.load(f)
            
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã
            examples = []
            for item in all_data:
                example = TrainingExample(
                    text=item['text'],
                    intent_label=item['intent_label'],
                    explanation=item['explanation'],
                    examples=item['examples']
                )
                examples.append(example)
                self.known_intents.add(item['intent_label'])
            
            # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
            self.dataset = CommandDataset(examples, self.vocab_size)
            
            # –û–±—É—á–∞–µ–º
            dataloader = DataLoader(
                self.dataset,
                batch_size=8,
                shuffle=True
            )
            
            self._train_epoch(dataloader, num_epochs=10)
            
            # –ü–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
            self.intent_embeddings = {}
            for example in examples:
                self._create_command_embedding(example)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
            self.save_model()
            
            print(f"‚úÖ –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –ó–Ω–∞—é {len(self.known_intents)} –Ω–∞–º–µ—Ä–µ–Ω–∏–π")
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è: {e}")
            return False

# –£—Ç–∏–ª–∏—Ç–∞ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
def test_learning_engine():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –¥–≤–∏–∂–æ–∫ –æ–±—É—á–µ–Ω–∏—è"""
    print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–≤–∏–∂–æ–∫–∞ –æ–±—É—á–µ–Ω–∏—è...")
    
    # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—É—Ç–∏
    import tempfile
    temp_dir = tempfile.mkdtemp()
    model_path = os.path.join(temp_dir, "test_model.pt")
    data_path = os.path.join(temp_dir, "test_data.json")
    
    # –°–æ–∑–¥–∞–µ–º –¥–≤–∏–∂–æ–∫
    engine = NeuralLearningEngine(model_path, data_path)
    
    # –¢–µ—Å—Ç–æ–≤–∞—è –∫–æ–º–∞–Ω–¥–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    test_command = "–æ—Ç–∫—Ä–æ–π –∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä"
    explanation = "–û—Ç–∫—Ä—ã—Ç—å –ø—Ä–æ–≥—Ä–∞–º–º—É –∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä –Ω–∞ –∫–æ–º–ø—å—é—Ç–µ—Ä–µ"
    examples = ["–∑–∞–ø—É—Å—Ç–∏ –∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä", "–≤–∫–ª—é—á–∏ –∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä", "–æ—Ç–∫—Ä–æ–π –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä"]
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø—Ä–æ—Å—Ç–æ–π –∫–æ–¥
    generated_code = '''
def execute():
    """–û—Ç–∫—Ä—ã–≤–∞–µ—Ç –∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä"""
    import subprocess
    try:
        subprocess.Popen("calc.exe")
        return "–ö–∞–ª—å–∫—É–ª—è—Ç–æ—Ä –æ—Ç–∫—Ä—ã—Ç"
    except Exception as e:
        return f"–û—à–∏–±–∫–∞: {str(e)}"
'''
    
    # –û–±—É—á–∞–µ–º
    success = engine.train_on_example(
        text=test_command,
        explanation=explanation,
        examples=examples,
        generated_code=generated_code
    )
    
    if success:
        print("‚úÖ –û–±—É—á–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –∫–æ–º–∞–Ω–¥
        similar = engine.find_similar_command("–∑–∞–ø—É—Å—Ç–∏ –∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä")
        if similar:
            print(f"‚úÖ –ù–∞–π–¥–µ–Ω–∞ –ø–æ—Ö–æ–∂–∞—è –∫–æ–º–∞–Ω–¥–∞: {similar['command']}")
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –Ω–∞–º–µ—Ä–µ–Ω–∏–µ
        intent, confidence = engine.predict_intent(test_command)
        print(f"‚úÖ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ –Ω–∞–º–µ—Ä–µ–Ω–∏–µ: {intent} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.2%})")
    
    # –û—á–∏—Å—Ç–∫–∞
    import shutil
    shutil.rmtree(temp_dir)
    print("üßπ –¢–µ—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã —É–¥–∞–ª–µ–Ω—ã")

if __name__ == "__main__":
    import time
    test_learning_engine()